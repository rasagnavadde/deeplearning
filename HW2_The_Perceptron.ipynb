{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-02-06 23:10:39--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2844 (2.8K)\n",
      "Saving to: 'test.dat.9'\n",
      "\n",
      "     0K ..                                                    100% 38.9M=0s\n",
      "\n",
      "2022-02-06 23:10:39 (38.9 MB/s) - 'test.dat.9' saved [2844/2844]\n",
      "\n",
      "--2022-02-06 23:10:39--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11244 (11K)\n",
      "Saving to: 'train.dat.9'\n",
      "\n",
      "     0K ..........                                            100% 1.25M=0.009s\n",
      "\n",
      "2022-02-06 23:10:39 (1.25 MB/s) - 'train.dat.9' saved [11244/11244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head test.dat\n",
    "!head train.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    result=0\n",
    "    for array1,array2 in zip(array1,array2):\n",
    "        result = result+array1*array2\n",
    "    return result\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    rs=1 / (1 + math.exp(-x))\n",
    "    return rs\n",
    "\n",
    "# The output of the model, which for the perceptron is\n",
    "# the sigmoid function applied to the dot product of\n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    result1=0\n",
    "    for weight,instance in zip(weight,instance):\n",
    "        result1 = result1+weight*instance\n",
    "    return result1\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #prediction of the model\n",
    "    activation = weights[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        activation += weights[i - 1] * instance[i]\n",
    "    return 1.0 if activation >= 0.5 else 0.0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate)\n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #updating weights\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            #dot product and sigmoid\n",
    "            in_value = dot_product(instance,weights)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            #looping and updating weights\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "The dot product, too called  scalar product, of two vector s could be a number ( Scalar amount) gotten by performing a particular operation on the vector components. The speck item has meaning as it were for sets of vectors having the same number of dimensions. The image for dot item may be a overwhelming dot ( ).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 61.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 52.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 65.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 51.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 43.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 61.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 54.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 39.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 35.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 61.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 43.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 52.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 52.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 38.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 54.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 35.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 61.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 40.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 31.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 54.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 42.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 35.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 33.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 61.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 43.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 31.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 53.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 31.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 49.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 39.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 34.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 42.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 36.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 34.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 37.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 36.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 31.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 38.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 35.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 33.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "    for tr_size in tr_percent:\n",
    "        for epochs in num_epochs:\n",
    "            size =  round(len(instances_tr)*tr_size/100)\n",
    "            pre_instances = instances_tr[0:size]\n",
    "            weights = train_perceptron(pre_instances, lr, epochs)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "              f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1591da16dc0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n",
    " -A.I did not utilize the same source dataset for test. I ought to do a appropriate train/test part in which both of them have the same basic dispersion. Most likely  given a totally diverse (and more pleasant) dataset for test an nonsensically tall degree of regularization was connected. Indeed so there would ought to be a few component of \"test information conveyance isn't the same as that of train\" for the watched behavior to occur.\n",
    " \n",
    " -B.Overfitting occurs when a show starts to center on the clamor within the preparing information set and extricates highlights based on it. This makes a difference the show to make strides its execution on the preparing set but harms its capacity to generalize so the precision on the approval set decreases.\n",
    " \n",
    " -C.Hyperparameters are movable parameters we select to prepare a demonstrate that administers the preparing handle itself.\n",
    " \n",
    " -D.In arrange to dodge overfitting here, preparing assist isn't recommended. In any case you will select to prepare the demonstrate past the age where preparing and approval exactness matches on the off chance that the coming about approval accuracy is adequate for the specific issue you're working on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
